diff --git a/backend/app/agents/agent_1.py b/backend/app/agents/agent_1.py
deleted file mode 100644
index 6b49a76..0000000
--- a/backend/app/agents/agent_1.py
+++ /dev/null
@@ -1,76 +0,0 @@
-from openai import OpenAI
-import os
-from .xiaohongshu_crawler import XiaohongshuCrawler
-
-
-SYSTEM_PROMPT = """
-
-"""
-class RootAgent:
-    def __init__(self):
-        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url="https://api.deepseek.com")
-        self.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
-        self.json_file = "todos.json"
-        self.xhs_crawler = XiaohongshuCrawler()
-
-    def generate_response(self, user_content):
-        self.messages.append({"role": "user", "content": user_content})
-        response = self.client.chat.completions.create(
-            model="deepseek-chat",
-            messages=self.messages,
-        )
-            
-        ai_content = response.choices[0].message.content 
-        self.messages.append({"role": "assistant", "content": ai_content})
-        return ai_content
-
-
-
-    def handle_user_message(self, user_content: str) -> dict:
-            user_content = user_content.strip()
-
-            # Step 1: 用大模型提取纯净小红书URL
-            prompt  = """你是一个小红书笔记URL提取助手。你的任务是从用户输入的内容中识别并提取出小红书笔记的纯净URL。
-
-                请严格按照以下规则执行：
-                1. 仔细扫描用户输入的全部文本内容
-                2. 识别出所有符合URL格式的链接）
-                3. 如果找到多个小红书笔记链接，只返回第一个
-                4. 只返回纯粹的URL链接
-                5. 如果找不到符合条件的小红书笔记URL，返回空字符串 ""
-
-                示例：
-                用户输入："帮我提取这个小红书笔记 https://www.xiaohongshu.com/explore/69327271000000001e021002?xsec_token=ABKPhNpGT-KmvTfRRpKLWNgxPMDCtRt6hjnTQ47mrC6cU=&xsec_source=pc_feed"
-                你返回："https://www.xiaohongshu.com/explore/69327271000000001e021002?xsec_token=ABKPhNpGT-KmvTfRRpKLWNgxPMDCtRt6hjnTQ47mrC6cU=&xsec_source=pc_feed"
-            
-
-
-                现在请处理当前用户输入："""
-
-            try:
-                response = self.client.chat.completions.create(
-                    model="deepseek-chat",
-                    messages=[
-                        {"role": "system", "content": prompt + user_content},
-                    ],
-                    temperature=0.0,
-                    max_tokens=100
-                )
-                url = response.choices[0].message.content.strip().strip('"')
-
-                # 严格校验格式
-                if url:  # 只要大模型返回了非空字符串，就直接用
-                    print(f"[Agent] 提取到链接: {url}")
-                    return self.xhs_crawler.crawl_note(url)
-                    
-
-            except Exception as e:
-                print(f"[提取URL失败] {e}")
-
-            # 没有提取到有效链接
-            return {
-                "reply": "未检测到有效的小红书笔记链接，请分享正确的笔记链接～"
-            }
-
-
-
diff --git a/backend/app/agents/xiaohongshu_crawler.py b/backend/app/agents/xiaohongshu_crawler.py
deleted file mode 100644
index 78be6cb..0000000
--- a/backend/app/agents/xiaohongshu_crawler.py
+++ /dev/null
@@ -1,222 +0,0 @@
-import re
-import asyncio
-from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
-from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
-from datetime import datetime
-from .video_download import VideoDownloader
-from .image_download import ImageDownloader
-import json
-import os
-from pathlib import Path
-from app.core.config import DATA_DIR
-from app.core.ids import new_uuid
-
-class XiaohongshuCrawler:
-
-    def __init__(self, data_dir=DATA_DIR):
-        self.data_dir = data_dir
-        self.records_path = self.data_dir / "records.jsonl"
-
-        os.makedirs(self.data_dir, exist_ok=True)
-        if not os.path.exists(self.records_path):
-            with open(self.records_path, "w", encoding="utf-8") as f:
-                pass
-    def _persist_result(self, data: dict):
-        with open(self.records_path, "a", encoding="utf-8") as f:
-            f.write(json.dumps(data, ensure_ascii=False) + "\n")
-
-    def _has_video_content(self, html: str) -> bool:
-        """
-        判断HTML中是否包含视频内容
-        :param html: 网页HTML
-        :return: True/False
-        """
-        # 检查视频关键词
-        video_patterns = [
-            r'"video"[^}]*"stream"[^}]*"h264"',
-            r'"media"[^}]*"video"[^}]*"stream"',
-            r'"masterUrl"[^}]*http[^"]+\.mp4',
-            r'"streamType"[^}]*"h264"',
-        ]
-        
-        for pattern in video_patterns:
-            if re.search(pattern, html, re.IGNORECASE):
-                return True
-        return False
-    
-
-    
-    def _extract_images(self, html: str):
-        """提取高清原图链接（去重，优先 prv）"""
-        pattern = r'http://sns-webpic-qc\.xhscdn\.com/[^"\s]+!nd_(?:prv|dft)_wlteh_webp_\d+'
-        urls = re.findall(pattern, html)
-        seen = {}
-        for url in urls:
-            key_match = re.search(r'(1040g[^!]+)', url)
-            if key_match:
-                key = key_match.group(1)
-                if key not in seen or ('!nd_prv_' in url and '!nd_prv_' not in seen[key]):
-                    seen[key] = url
-        return list(seen.values())
-
-    def _parse_content(self, html: str, source_url: str):
-        """解析页面内容，返回标准格式"""
-        info = {
-            "source_url": source_url,
-            "source_platform": "小红书",
-            "author_name": "",
-            "words": "",
-            "videos": [],
-            "images": []
-        }
-
-        # 作者名
-        for pat in [
-            r'class="username"[^>]*>([^<]+)</span>',
-            r'"nickname":"([^"]+)"',
-            r'"user"[^{]*"nickname":"([^"]+)"',
-            r'class="name"[^>]*>([^<]+)</a>'
-        ]:
-            m = re.search(pat, html)
-            if m and m.group(1).strip() not in ["", "小红书"]:
-                info["author_name"] = m.group(1).strip()
-                break
-
-        # 标题
-        title = re.search(r'<div[^>]*id="detail-title"[^>]*>(.*?)</div>',  html)
-        title = title.group(1) if title else ""
-
-        # 正文
-        desc = re.search(r'"desc":"([^"]+)"', html)
-        desc_text = ""
-        if desc:
-            desc_text = desc.group(1).replace('\\n', '\n').replace('\\t', ' ')
-            desc_text = re.sub(r'#[^[]+\[话题\]\s*', '', desc_text).strip()
-
-        # 时间地点
-        location = re.search(r'"ipLocation":"([^"]+)"', html)
-        location = location.group(1) if location else ""
-
-        time_match = re.search(r'"time":(\d+)', html)
-        date_str = ""
-        if time_match:
-            try:
-                ts = int(time_match.group(1)) // 1000
-                date_str = datetime.fromtimestamp(ts).strftime("%m-%d")
-            except:
-                pass
-        date_location = f"{date_str} {location}".strip() if date_str or location else ""
-
-        # 标签
-        tags = " ".join(f"#{t}" for t in re.findall(r'#([\w\u4e00-\u9fa5]+)\[话题\]', html))
-
-        # 组合文字
-        parts = [p for p in [title, desc_text, date_location, tags] if p]
-        info["words"] = re.sub(r'\s+', ' ', " ".join(parts)).strip()
-
-        # 图片原始链接
-        info["images"] = self._extract_images(html)
-
-        return info
-
-    async def _fetch_html(self, url: str) -> str:
-        headers = {
-            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) "
-                           "AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148 XiaoHongShu/8.25.0"
-        }
-        config = CrawlerRunConfig(
-            scraping_strategy=LXMLWebScrapingStrategy(),
-            verbose=False
-        )
-        async with AsyncWebCrawler() as crawler:
-            result = await crawler.arun(url=url, config=config, magic=True, headers=headers)
-            if not result.success or not result.html:
-                raise Exception("页面爬取失败或内容为空")
-            return result.html
-
-    async def crawl_note(self, clean_url: str) -> dict:
-        """
-        主方法：接收一个纯净的笔记 URL，返回解析结果
-        """
-
-        try:
-            html = await self._fetch_html(clean_url)
-            data = self._parse_content(html, clean_url)
-            print(html)
-
-            # 先生成 record_id，后续下载/落盘都按该目录组织
-            data["record_id"] = data.get("record_id") or new_uuid()
-            record_dir = self.data_dir / data["record_id"]
-            os.makedirs(record_dir, exist_ok=True)
-
-            # 下载图片，并把相对路径写回 images 字段
-            image_urls = data.get("images") or []
-            if image_urls:
-                saved_images = []
-                img_downloader = ImageDownloader(record_dir)
-                for url in image_urls:
-                    try:
-                        filename = img_downloader.download_image(url)
-                        saved_images.append((Path(data["record_id"]) / filename).as_posix())
-                    except Exception as e:
-                        print(f"[XiaohongshuCrawler] 图片下载失败: {url} | {str(e)}")
-                data["images"] = saved_images
-
-            # 检测是否有视频内容
-            if self._has_video_content(html):
-                print(f"[XiaohongshuCrawler] 检测到视频内容，开始下载...")
-                
-                # 直接下载原始URL
-                try:
-                    downloader = VideoDownloader(record_dir)
-                    filename = downloader.download_video(clean_url)
-                    if filename:
-                        # 存相对路径，便于从 DATA_DIR 定位文件
-                        data["videos"].append((Path(data["record_id"]) / filename).as_posix())
-                        print(f"[XiaohongshuCrawler] 视频下载成功: {filename}")
-                    else:
-                        print(f"[XiaohongshuCrawler] 视频下载失败")
-                except Exception as e:
-                    print(f"[XiaohongshuCrawler] 视频下载异常: {str(e)}")
-            else:
-                print(f"[XiaohongshuCrawler] 未检测到视频内容")
-                data["videos"] = []
-            
-            self._persist_result(data)
-            reply_message = f"笔记下载完成，作者：{data['author_name']}\n{data['words']}"
-            return {
-                "reply": reply_message
-            }
-
-        except Exception as e:
-            error_message = f"爬取失败: {str(e)}"
-            print(error_message)
-            return {
-                "reply": error_message
-            }
-
-def main():
-    # 从 backend/test/urls.txt 挑的样例（可自行增删）
-    test_urls = [
-        "https://www.xiaohongshu.com/explore/69521554000000001e035658?xsec_token=ABkzg2CYlgCu419P_iDdwgK5O-MlNln5-UiXUxZHfUzEw=&xsec_source=pc_feed",
-        "https://www.xiaohongshu.com/explore/696204f1000000002200bfd6?xsec_token=ABCvIn39KwblGPS9HmxFV8On6azZyjm_DIB-_kwGRvjPE=&xsec_source=pc_feed",
-        "https://www.xiaohongshu.com/explore/6957c911000000001e0052a6?xsec_token=ABLLDxF81CLnALitGMZ3TI6La8RH3MsJqdj3cxgCaBTco=&xsec_source=pc_cfeed",
-        "https://www.xiaohongshu.com/explore/69627d9f000000000a02a407?xsec_token=ABCvIn39KwblGPS9HmxFV8OoH0D6lZxsR9p2iZOsv5Uik=&xsec_source=pc_cfeed"
-    ]
-
-    crawler = XiaohongshuCrawler(data_dir=DATA_DIR)
-
-    async def _run():
-        for url in test_urls:
-            print("\n" + "=" * 80)
-            print(f"[TEST] {url}")
-
-            # 小红书：解析图文；如检测到视频会触发下载并写入 data["videos"]
-            result = await crawler.crawl_note(url)
-            print(result)
-
-    asyncio.run(_run())
-
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/backend/main.py b/backend/main.py
index 5baffe8..8ad6580 100644
--- a/backend/main.py
+++ b/backend/main.py
@@ -3,7 +3,7 @@ from openai import OpenAI
 from pydantic import BaseModel
 from typing import List
 from dotenv import load_dotenv
-from app.agents.agent_1 import RootAgent
+from app.agents.agent_note_manager import NoteManager
 from fastapi.middleware.cors import CORSMiddleware
 from app.agents.Text_to_Image import ImageGenerator
 import os
@@ -12,8 +12,7 @@ import os
 load_dotenv()
 
 app = FastAPI(title="Inspiration Library AI", version="1.0.0")
-assistant = RootAgent()
-image_service = ImageGenerator()
+note_manager = NoteManager()
 app.add_middleware(
     CORSMiddleware,
     allow_origins=["http://localhost:3000", "http://192.168.1.3:3000"], # 允许前端地址
@@ -33,11 +32,8 @@ class Message(BaseModel):
 @app.post("/api/chat")
 async def chat(request: Message):
     user_content = request.content
-    if request.agent_id == "agent1":
-        result = await assistant.handle_user_message(user_content)
-        return result
-    elif request.agent_id == "agent2":
-        result = await image_service.generate_and_wait(user_content)
+    if request.agent_id == "note_manager":
+        result = await note_manager.handle_user_message(user_content)
         return result
 
 if __name__ == "__main__":
diff --git a/frontend/app/agent1/page.tsx b/frontend/app/agent1/page.tsx
index b5c1f99..b506895 100644
--- a/frontend/app/agent1/page.tsx
+++ b/frontend/app/agent1/page.tsx
@@ -5,7 +5,7 @@ import { ChatMessage } from "@/components/features/chat/ChatMessage"
 import { ChatInput } from "@/components/features/chat/ChatInput"
 
 export default function AgentPage() {
-	const { input, setInput, messages, handleSend } = useChat({ agentId: "agent1" })
+	const { input, setInput, messages, handleSend } = useChat({ agentId: "note_manager" })
 	return (
 		<div className="h-full flex flex-grow flex-col border">
 			<ChatMessage messages={messages} />
